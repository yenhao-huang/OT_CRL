{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULdrhOaVbsdO"
      },
      "source": [
        "\u003cimg src=\"https://raw.githubusercontent.com/deepmind/acme/master/docs/imgs/acme.png\" width=\"50%\"\u003e\n",
        "\n",
        "# Acme quickstart guide\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepmind/acme/blob/master/examples/quickstart.ipynb)\n",
        "\n",
        "This tutorial will walk you through how to construct a basic agent in Acme, define an environment to run that agent in, and finally to define the network architecture that acts as the *glue* between the agent and the environment.\n",
        "\n",
        "To make running experiments as easy as possible, Acme provides a high level\n",
        "interface which takes these three components and will run and evaluate the agent. See the figure below for a high-level illustration of this process. \n",
        "\u003c!-- If you would like more information on what lies behind this function, take a look at our RL tutorial which walks through the details of how the agent interacts with the environment. --\u003e\n",
        "\n",
        "\u003cimg src=\"https://raw.githubusercontent.com/deepmind/acme/master/docs/imgs/configure-and-run-experiments.png\" width=\"50%\"\u003e\n",
        "\n",
        "\n",
        "In the next few sections we will instantiate and run the D4PG algorithm on a continuous control environment, show the behavior of a trained agent, and plot the performance of this agent over time. While this tutorial is specific to this agent and environment, the same rough, high-level approach can be used for any other Acme algorithm!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75sdtBAtzgus"
      },
      "source": [
        "# Install Acme and import Modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbWISBQ6onzh"
      },
      "outputs": [],
      "source": [
        "%pip install git+https://github.com/deepmind/acme.git#egg=dm-acme[jax,tf,envs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoS8yzqstoXG"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "import collections\n",
        "from dm_control import suite as dm_suite\n",
        "import dm_env\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from acme import specs\n",
        "from acme import wrappers\n",
        "from acme.agents.jax import d4pg\n",
        "from acme.jax import experiments\n",
        "from acme.utils import loggers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff8p3IIeklfS"
      },
      "source": [
        "# Defining our environment\n",
        "\n",
        "Next we will need to define the environment in which to run the agent, implemented as a `dm_env.Environment`. In particular, here we will define an environment factory so that we can construct it later (when we run the experiment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSrQ9wHuwnnB"
      },
      "outputs": [],
      "source": [
        "def make_environment(seed: int) -\u003e dm_env.Environment:\n",
        "  environment = dm_suite.load('cartpole', 'balance')\n",
        "\n",
        "  # Make the observations be a flat vector of all concatenated features.\n",
        "  environment = wrappers.ConcatObservationWrapper(environment)\n",
        "\n",
        "  # Wrap the environment so the expected continuous action spec is [-1, 1].\n",
        "  # Note: this is a no-op on 'control' tasks.\n",
        "  environment = wrappers.CanonicalSpecWrapper(environment, clip=True)\n",
        "\n",
        "  # Make sure the environment outputs single-precision floats.\n",
        "  environment = wrappers.SinglePrecisionWrapper(environment)\n",
        "\n",
        "  return environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aISx0WIE0bCB"
      },
      "source": [
        "# Creating our neural networks \n",
        "\n",
        "Algorithms in Acme typically define a *Networks* datastructure, where networks are responsible for consuming outputs from the environment and translating those into quantities that will be used by the given algorithm. For example, D4PG makes use of\n",
        "\n",
        "* a deterministic Policy network and\n",
        "* a distributional Critic network\n",
        "\n",
        "which are used both for learning and to generate actions. In this case these are defined using the `d4pg.D4PGNetworks` dataclass. Most algorithms define a\n",
        "*default* set of networks, and so does D4PG as it defines `d4pg.make_networks`. \n",
        "These factories should return networks with appropriate input/output sizes\n",
        "given an `EnvironmentSpec` which specifies the sizes expected by the\n",
        "environment.\n",
        "\n",
        "Below we define a custom factory based on the default factory and adjust the\n",
        "sizes of hidden layers in the MLPs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hQlmlPKs8CN"
      },
      "outputs": [],
      "source": [
        "def network_factory(spec: specs.EnvironmentSpec) -\u003e d4pg.D4PGNetworks:\n",
        "  return d4pg.make_networks(\n",
        "      spec,\n",
        "      # These correspond to sizes of the hidden layers of an MLP.\n",
        "      policy_layer_sizes=(256, 256),\n",
        "      critic_layer_sizes=(256, 256),\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7P9sF080rhz"
      },
      "source": [
        "# Configuring the D4PGBuilder\n",
        "\n",
        "An agent `Builder` defines the different components, losses, etc. that make up an agent. However, at a high level we can think of this as defining the entire agent algorithm (apart from the networks defined earlier).\n",
        "\n",
        "Builders will also often take a configuration object that defines algorithm-specific hyperparameters. Here we define a `D4PGConfig` object which includes the learning rate and exploration noise standard deviation `sigma`,\n",
        "and then use that to define a `D4PGBuilder` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0eXq8F60kGn"
      },
      "outputs": [],
      "source": [
        "d4pg_config = d4pg.D4PGConfig(learning_rate=3e-4, sigma=0.2)\n",
        "d4pg_builder = d4pg.D4PGBuilder(d4pg_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OlfLTgrhFLk"
      },
      "source": [
        "# Logging our results\n",
        "\n",
        "We have also introduced a `Logger` object which collects the training and evaluation data and saves it in-memory. We will later use that to plot the results. Feel free to explore other loggers available in Acme!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byvSVa8JQL2C"
      },
      "outputs": [],
      "source": [
        "# Specify how to log training data: in this case keeping it in memory.\n",
        "# NOTE: We create a dict to hold the loggers so we can access the data after\n",
        "# the experiment has run.\n",
        "logger_dict = collections.defaultdict(loggers.InMemoryLogger)\n",
        "def logger_factory(\n",
        "    name: str,\n",
        "    steps_key: Optional[str] = None,\n",
        "    task_id: Optional[int] = None,\n",
        ") -\u003e loggers.Logger:\n",
        "  del steps_key, task_id\n",
        "  return logger_dict[name]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2zAiX-inBcR"
      },
      "source": [
        "# Configuring and running our first experiment\n",
        "\n",
        "Given all the components defined above we can now use them to run an experiment. We will first collect these components into an `ExperimentConfig` object which defines the experiment and then use `run_experiment` to run it. This will train the agent, and periodically evaluate it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lm8gRKnhZ_f"
      },
      "outputs": [],
      "source": [
        "experiment_config = experiments.ExperimentConfig(\n",
        "    builder=d4pg_builder,\n",
        "    environment_factory=make_environment,\n",
        "    network_factory=network_factory,\n",
        "    logger_factory=logger_factory,\n",
        "    seed=0,\n",
        "    max_num_actor_steps=50_000)  # Each episode is 1000 steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NAMKQfOn19i"
      },
      "source": [
        "Now we can run our experiment. This may take a few minutes to complete.\n",
        "\n",
        "Note 1: If you're in a particularly impatient mood, consider reducing\n",
        "`max_num_actor_steps` above, which will train for fewer environment\n",
        "interactions.\n",
        "\n",
        "Note 2: Alternatively you can reduce either `eval_every` or \n",
        "`num_eval_episodes`; the former specifies how long (in units of environment\n",
        "steps) between evaluation events, while the latter specifies the number of\n",
        "repeated evaluations at those times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro2rAKt5svSM"
      },
      "outputs": [],
      "source": [
        "experiments.run_experiment(\n",
        "    experiment=experiment_config,\n",
        "    eval_every=1000,\n",
        "    num_eval_episodes=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2vl_uYrktD9"
      },
      "source": [
        "# Plot the results\n",
        "\n",
        "Given the the data generated during training and logged using the `InMemoryLogger` we can now plot the performance as a function of the training steps. Note that DM control suite tasks have normalized scores and default to\n",
        "1000 step episodes, meaning that 1000 is also the maximum possible score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-bq9N9DZxS1"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "df = pd.DataFrame(logger_dict['evaluator'].data)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.title('Training episodes returns')\n",
        "plt.xlabel('Training episodes')\n",
        "plt.ylabel('Episode return')\n",
        "plt.plot(df['actor_episodes'], df['episode_return'], label='Training Episodes return')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "name": "Acme quickstart",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
