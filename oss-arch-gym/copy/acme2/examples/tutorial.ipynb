{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s26y3cu66W1n"
      },
      "source": [
        "\u003cimg src=\"https://raw.githubusercontent.com/deepmind/acme/master/docs/imgs/acme.png\" width=\"50%\"\u003e\n",
        "\n",
        "# Building an Acme agent (D4PG) manually\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepmind/acme/blob/master/examples/tutorial.ipynb)\n",
        "\n",
        "In this tutorial colab, we'll take a more in-depth look at Acme components by not\n",
        "using the `D4PGBuilder` nor the `run_experiment` function and building the\n",
        "agent's components and connecting them manually.\n",
        "\n",
        "This colab is best run with a GPU runtime, and in particular, the last cell will\n",
        "not run without it. This should be the default in this colab but in case it\n",
        "isn't, you can select a GPU runtime, by clicking on\n",
        "`Runtime \u003e Change runtime type` and then selecting `GPU` from the\n",
        "`Hardware type` dropdown menu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-H2d6UZi7Sf"
      },
      "source": [
        "# Install Acme and import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhmQaIfNoZXB"
      },
      "outputs": [],
      "source": [
        "%pip install git+https://github.com/deepmind/acme.git#egg=dm-acme[jax,tf,envs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQebRHQ2PcJ6"
      },
      "outputs": [],
      "source": [
        "%env MUJOCO_GL=egl\n",
        "from typing import Sequence, Tuple\n",
        "\n",
        "from dm_control import suite\n",
        "from dm_control.rl import control\n",
        "from IPython.display import HTML\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import haiku as hk\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import optax\n",
        "import reverb\n",
        "import rlax\n",
        "import tensorflow as tf\n",
        "\n",
        "import acme\n",
        "from acme import specs\n",
        "from acme import wrappers\n",
        "from acme.adders import reverb as reverb_adders\n",
        "from acme.agents.jax import actors\n",
        "from acme.agents.jax import actor_core as actor_core_lib\n",
        "from acme.agents.jax.d4pg import learning\n",
        "from acme.datasets import reverb as datasets\n",
        "from acme.jax import utils, variable_utils\n",
        "from acme.jax import networks as networks_lib\n",
        "from acme.jax.experiments.run_experiment import _disable_insert_blocking, _LearningActor\n",
        "from acme.utils import counting\n",
        "from acme.utils import loggers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXmqZCKsbMRI"
      },
      "source": [
        "# Configure the D4PG agent\n",
        "\n",
        "Next we must make hyperparameter choices for the agent as these \n",
        "will affect the neural network creation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_pGhXAlfgKU"
      },
      "outputs": [],
      "source": [
        "key = jax.random.PRNGKey(123)\n",
        "\n",
        "batch_size = 256\n",
        "learning_rate = 1e-4\n",
        "discount = 0.99\n",
        "n_step = 5  # The D4PG agent learns from n-step transitions.\n",
        "exploration_sigma = 0.3\n",
        "target_update_period = 100\n",
        "\n",
        "# Controls the relative rate of sampled vs inserted items. In this case, items\n",
        "# are n-step transitions.\n",
        "samples_per_insert = 32.0\n",
        "\n",
        "# Atoms used by the categorical distributional critic.\n",
        "num_atoms = 51\n",
        "critic_atoms = jnp.linspace(-150., 150., num_atoms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6KuVGSk4uc9"
      },
      "source": [
        "# Load the environment\n",
        "\n",
        "We first load our desired environment and wrap it to process its input and\n",
        "output. See the inline comments to inspect the purpose of the individual\n",
        "wrappers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PVlHtGF5yzt"
      },
      "outputs": [],
      "source": [
        "# Control suite environments are dm_env.Environments with additional attributes\n",
        "# such as a `physics` object, which we use to render the scene.\n",
        "environment: control.Environment = suite.load('cartpole', 'balance')\n",
        "\n",
        "# Concatenate the observations (position, velocity, etc).\n",
        "environment = wrappers.ConcatObservationWrapper(environment)\n",
        "\n",
        "# Make the environment expect continuous action spec is [-1, 1].\n",
        "# Note: this is a no-op on dm_control tasks.\n",
        "environment = wrappers.CanonicalSpecWrapper(environment, clip=True)\n",
        "\n",
        "# Make the environment output single-precision floats.\n",
        "# We use this because most TPUs only work with float32.\n",
        "environment = wrappers.SinglePrecisionWrapper(environment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ8jsGhi7gkc"
      },
      "source": [
        "Finally, we grab its specifications (shapes\n",
        "and dtypes of observations/actions/rewards/discount), which will be necessary\n",
        "for specifying the neural network input and output shapes as well as the replay\n",
        "buffer item signature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LAuAHhi71Vy"
      },
      "outputs": [],
      "source": [
        "environment_spec = specs.make_environment_spec(environment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kduyK4zpbcdE"
      },
      "source": [
        "# Create the Haiku networks\n",
        "\n",
        "Here we recreate the D4PG default neural networks of the same size as those used\n",
        "in the quickstart colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtDV5nggbfX7"
      },
      "outputs": [],
      "source": [
        "# Calculate how big the last layer should be based on total # of actions.\n",
        "action_spec = environment_spec.actions\n",
        "action_size = np.prod(action_spec.shape, dtype=int)\n",
        "\n",
        "# Create the deterministic policy network.\n",
        "def policy_fn(obs: networks_lib.Observation) -\u003e jnp.ndarray:\n",
        "  x = obs\n",
        "  x = networks_lib.LayerNormMLP([256, 256], activate_final=True)(x)\n",
        "  x = networks_lib.NearZeroInitializedLinear(action_size)(x)\n",
        "  x = networks_lib.TanhToSpec(action_spec)(x)\n",
        "  return x\n",
        "\n",
        "# Create the distributional critic network.\n",
        "def critic_fn(\n",
        "    obs: networks_lib.Observation,\n",
        "    action: networks_lib.Action,\n",
        ") -\u003e Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "  x = jnp.concatenate([obs, action], axis=-1)\n",
        "  x = networks_lib.LayerNormMLP(layer_sizes=[256, 256, num_atoms])(x)\n",
        "  return x, critic_atoms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W50_VymuIfGS"
      },
      "source": [
        "This is how Haiku transforms the simple functions above into two functions:\n",
        "- init, which returns the parameters and\n",
        "- apply, which applies the neural network to inputs given parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Pab27W5Idis"
      },
      "outputs": [],
      "source": [
        "policy = hk.without_apply_rng(hk.transform(policy_fn))\n",
        "critic = hk.without_apply_rng(hk.transform(critic_fn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn4hFsA6BrWa"
      },
      "source": [
        "In order to create the neural network parameters, we create dummy versions of\n",
        "the environments observations and actions. This allows us to prebind the network\n",
        "`init` methods so that only a random key is needed to initialize parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CxCZDPzbfVj"
      },
      "outputs": [],
      "source": [
        "# Create dummy observations and actions to create network parameters.\n",
        "dummy_action = utils.zeros_like(environment_spec.actions)\n",
        "dummy_obs = utils.zeros_like(environment_spec.observations)\n",
        "\n",
        "# Prebind dummy observations and actions so they are not needed in the learner.\n",
        "policy_network = networks_lib.FeedForwardNetwork(\n",
        "    init=lambda rng: policy.init(rng, dummy_obs),\n",
        "    apply=policy.apply)\n",
        "critic_network = networks_lib.FeedForwardNetwork(\n",
        "    init=lambda rng: critic.init(rng, dummy_obs, dummy_action),\n",
        "    apply=critic.apply)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4yPM6U6JTZ2"
      },
      "source": [
        "Finally we create an exploration policy by simply wrapping the policy network\n",
        "to add Gaussian noise with the exploration sigma specified in the \"Configure\n",
        "the D4PG agent\" cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiToF1V5vAJq"
      },
      "outputs": [],
      "source": [
        "def exploration_policy(\n",
        "    params: networks_lib.Params,\n",
        "    key: networks_lib.PRNGKey,\n",
        "    observation: networks_lib.Observation,\n",
        ") -\u003e networks_lib.Action:\n",
        "  action = policy_network.apply(params, observation)\n",
        "  if exploration_sigma:\n",
        "    action = rlax.add_gaussian_noise(key, action, exploration_sigma)\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BukOfOsmtSQn"
      },
      "source": [
        "# Create a D4PG agent components\n",
        "\n",
        "In this section we create the agent components manually one by\n",
        "one. Note that this is usually done by the `run_experiment` or\n",
        "`make_distributed_experiment` script but for the purposes of\n",
        "this tutorial we create and use them explicitly.\n",
        "\n",
        "The run scripts make use of the agent builder (in this case\n",
        "`D4PGBuilder`), which we don't use here since this tutorial is\n",
        "partially meant to peel this layer of abstraction to demystify\n",
        "the builder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wObU6ZFtOzwB"
      },
      "source": [
        "## Create a central counter\n",
        "\n",
        "This is the parent counter to which all other component counters\n",
        "will synchronize their counts (of their corresponding steps,\n",
        "walltimes, etc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6Hb0v3kOwSh"
      },
      "outputs": [],
      "source": [
        "parent_counter = counting.Counter(time_delta=0.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0DZinzbgsXH"
      },
      "source": [
        "## Create the replay table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlSb9EoReqQV"
      },
      "outputs": [],
      "source": [
        "# Manages the data flow by limiting the sample and insert calls.\n",
        "rate_limiter = reverb.rate_limiters.SampleToInsertRatio(\n",
        "    min_size_to_sample=1000,\n",
        "    samples_per_insert=samples_per_insert,\n",
        "    error_buffer=2 * batch_size)\n",
        "\n",
        "# Create a replay table to store previous experience.\n",
        "replay_tables = [\n",
        "    reverb.Table(\n",
        "        name='priority_table',\n",
        "        sampler=reverb.selectors.Uniform(),\n",
        "        remover=reverb.selectors.Fifo(),\n",
        "        max_size=1_000_000,\n",
        "        rate_limiter=rate_limiter,\n",
        "        signature=reverb_adders.NStepTransitionAdder.signature(\n",
        "            environment_spec))\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2oSmGKGPvV_"
      },
      "source": [
        "In single-process execution, acting (data generation and _insertion_) and \n",
        "learning (_sampling_ and data consumption) is done sequentially.\n",
        "\n",
        "Below we make sure that consumption happens only when enough data (a batch) is\n",
        "ready, otherwise acting continues. We avoid the actor's inserts blocking by\n",
        "disabling Reverb's rate limitation in one direction (inserts).\n",
        "\n",
        "***Note!*** *This is the first of three code cells that are specific to\n",
        "single-process execution. (This is done for you when you use an agent `Builder`\n",
        "and `run_experiment`.) Everything else is logic shared between the two.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff0NMQHQPfnQ"
      },
      "outputs": [],
      "source": [
        "# NOTE: This is the first of three code cells that are specific to\n",
        "# single-process execution. (This is done for you when you use an agent\n",
        "# `Builder` and `run_experiment`.) Everything else is logic shared between the\n",
        "# two.\n",
        "replay_tables, rate_limiters_max_diff = _disable_insert_blocking(replay_tables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgKvkHokPhUG"
      },
      "source": [
        "Finally we create the replay buffer server and client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haDUGlrGPgPt"
      },
      "outputs": [],
      "source": [
        "replay_server = reverb.Server(replay_tables, port=None)\n",
        "replay_client = reverb.Client(f'localhost:{replay_server.port}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QF3lgso9ZdK"
      },
      "source": [
        "## Create the learner's dataset iterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU-U1Pc6mdh6"
      },
      "outputs": [],
      "source": [
        "# Pull data from the Reverb server into a TF dataset the agent can consume.\n",
        "dataset = datasets.make_reverb_dataset(\n",
        "    table='priority_table',\n",
        "    server_address=replay_client.server_address,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "# We use multi_device_put here in case this colab is run on a machine with\n",
        "# multiple accelerator devices, but this works fine with single-device learners\n",
        "# as long as their step functions are pmapped.\n",
        "dataset = utils.multi_device_put(dataset.as_numpy_iterator(), jax.devices())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWDUSL2TFAKz"
      },
      "source": [
        "In single-process execution we must always use this `utils.prefetch` function.\n",
        "\n",
        "***Note!*** *This is the second of three code cells that are specific to\n",
        "single-process execution. (This is done for you when you use an agent `Builder`\n",
        "and `run_experiment`.) Everything else is logic shared between the two.*\n",
        "\n",
        "This utility function produces an identical iterator with an\n",
        "additional `.ready()` method. The `_LearningActor` (only to be\n",
        "used within `run_experiment`) then uses the `iterator.ready()`\n",
        "method to avoid deadlocks in single-process execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkTqKPxWEq_b"
      },
      "outputs": [],
      "source": [
        "# NOTE: This is the second of three code cells that are specific to\n",
        "# single-process execution. (This is done for you when you use an agent\n",
        "# `Builder` and `run_experiment`.) Everything else is logic shared between the\n",
        "# two.\n",
        "dataset = utils.prefetch(dataset, buffer_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-OAMQPMgU9o"
      },
      "source": [
        "## Create the learner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC2xTRp0mhYu"
      },
      "outputs": [],
      "source": [
        "key, learner_key = jax.random.split(key)\n",
        "\n",
        "# The learner updates the parameters (and initializes them).\n",
        "learner = learning.D4PGLearner(\n",
        "    policy_network=policy_network,\n",
        "    critic_network=critic_network,\n",
        "    random_key=learner_key,\n",
        "    policy_optimizer=optax.adam(learning_rate),\n",
        "    critic_optimizer=optax.adam(learning_rate),\n",
        "    discount=discount,\n",
        "    target_update_period=target_update_period,\n",
        "    iterator=dataset,\n",
        "    # A simple counter object that can periodically sync with a parent counter.\n",
        "    counter=counting.Counter(parent_counter, prefix='learner', time_delta=0.),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRggQU8lgQWi"
      },
      "source": [
        "## Create the adder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DxEMsJqOR9-"
      },
      "outputs": [],
      "source": [
        "# Handles preprocessing of data and insertion into replay tables.\n",
        "adder = reverb_adders.NStepTransitionAdder(\n",
        "    priority_fns={'priority_table': None},\n",
        "    client=replay_client,\n",
        "    n_step=n_step,\n",
        "    discount=discount)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP4jl0A9gWyE"
      },
      "source": [
        "## Create the actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mriO9Q_OnpwS"
      },
      "outputs": [],
      "source": [
        "key, actor_key = jax.random.split(key)\n",
        "\n",
        "# A convenience adaptor from FeedForwardPolicy to ActorCore.\n",
        "actor_core = actor_core_lib.batched_feed_forward_to_actor_core(\n",
        "    exploration_policy)\n",
        "\n",
        "# A variable client for updating variables from a remote source.\n",
        "variable_client = variable_utils.VariableClient(learner, 'policy', device='cpu')\n",
        "actor = actors.GenericActor(\n",
        "    actor=actor_core,\n",
        "    random_key=actor_key,\n",
        "    variable_client=variable_client,\n",
        "    adder=adder,\n",
        "    backend='cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anBIIkKiHXFx"
      },
      "source": [
        "Replace the actor with a `_LearningActor`.\n",
        "\n",
        "***Note!*** *This is the third of three code cells that are specific to\n",
        "single-process execution. (This is done for you when you use an agent `Builder`\n",
        "and `run_experiment`.) Everything else is logic shared between the two.*\n",
        "\n",
        "Every `update` call, the `_LearningActor` checks whether there is enough new data to learn from; if so it runs a learner step,\n",
        "otherwise it cedes control back to the environment loop.\n",
        "Thus it avoids deadlocks whereby the learner is asked to update\n",
        "but the iterator does not have a batch of data ready for it.\n",
        "\n",
        "As usual the rate at which new data is released is controlled\n",
        "by the replay table's rate_limiter which is created by the\n",
        "builder.make_replay_tables call above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLoig9UPcYj7"
      },
      "outputs": [],
      "source": [
        "# NOTE: This is the third of three code cells that are specific to\n",
        "# single-process execution. (This is done for you when you use an agent\n",
        "# `Builder` and `run_experiment`.) Everything else is logic shared between the\n",
        "# two.\n",
        "actor = _LearningActor(actor, learner, dataset, replay_tables,\n",
        "                       rate_limiters_max_diff, checkpointer=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-NPswqYORui"
      },
      "outputs": [],
      "source": [
        "env_loop_logger = loggers.InMemoryLogger()\n",
        "\n",
        "# Create the environment loop used for training.\n",
        "env_loop = acme.EnvironmentLoop(\n",
        "    environment,\n",
        "    actor,\n",
        "    counter=counting.Counter(parent_counter, prefix='train', time_delta=0.),\n",
        "    logger=env_loop_logger)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKeGQxzitXYC"
      },
      "source": [
        "# Run a training loop\n",
        "\n",
        "This may take a while... If you're feeling impatient consider reducing the\n",
        "number of episodes down to 5 in the following cell and rerun it until the\n",
        "plot in the next cell approaches the top score of 1000. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWZd5N-Qoz82"
      },
      "outputs": [],
      "source": [
        "env_loop.run(num_episodes=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aozZPAf50-c"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "df = pd.DataFrame(env_loop_logger.data)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.title('Training episodes returns')\n",
        "plt.xlabel('Training episodes')\n",
        "plt.ylabel('Episode return')\n",
        "plt.plot(df['episode_return']);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh8x46UKSf2c"
      },
      "source": [
        "# Run and visualize the agent in the environment\n",
        "\n",
        "For this run we wrap the environment so that every timestep is rendered allowing\n",
        "us to create and display a video.\n",
        "\n",
        "**Warning!** Rendering is expensive! Make sure you don't wrap the environment\n",
        "used for gathering data in the training loop above. We name this environment\n",
        "differently in case you want to run the training loop again after visualizing\n",
        "the agent here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2__mFiraWND1"
      },
      "outputs": [],
      "source": [
        "# Make the environment render frames and produce videos of episodes.\n",
        "eval_environment = wrappers.MujocoVideoWrapper(environment, record_every=1)\n",
        "\n",
        "timestep = eval_environment.reset()\n",
        "\n",
        "while not timestep.last():\n",
        "  action = actor.select_action(timestep.observation)\n",
        "  timestep = eval_environment.step(action)\n",
        "\n",
        "# Embed the HTML video.\n",
        "HTML(eval_environment.make_html_animation())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "name": "Acme tutorial",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
